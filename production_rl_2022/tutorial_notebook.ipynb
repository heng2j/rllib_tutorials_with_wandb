{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy trained models to production using Ray Serve.\n",
    "\n",
    "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "$ # **Note:** In case you are getting a \"requires TensorFlow version >= 2.8\" error at some point in the notebook, try the following:\n",
    "$ pip uninstall -y tensorflow\n",
    "$ python -m pip install tensorflow-macos --no-cache-dir\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install pywin32 # <- extra install only on Win10.\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/production_rl_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* What's offline RL and how can I use it with RLlib?\n",
    "* How do I deploy an already trained policy into production using Ray Serve.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. Reinforcement learning (RL) in a nutshell.\n",
    "1. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work.\n",
    "1. Why you should use RLlib.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. [Google RecSim - Build your own recom sys simulator.](#recsim)\n",
    "1. [Dissecting the \"long term satisfaction\" (LTE) environment.](#dissecting_lte)\n",
    "1. [Using a contextual Bandit algorithm with RLlib and starting our first training run on the LTE env.](#rllib)\n",
    "1. [What did the Bandit learn?](#bandit_results)\n",
    "1. [Intro to Slate-Q.](#slateq)\n",
    "1. [Starting a Slate-Q training run.](#slateq_experiment)\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. [Analyzing the results of the SlateQ run.](#slateq_results)\n",
    "1. [Intro to Offline RL - What if we don't have an environment?](#offline_rl)\n",
    "1. [BC (behavior cloning) and MARWIL: Quick how-to and setup instructions.](#bc_and_marwil)\n",
    "1. [Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.](#ope)\n",
    "1. [Ray Serve example: How can we deploy a trained policy into our production environment?](#ray_serve)\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import wandb\n",
    "\n",
    "import ray  # .. of course\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.integration.wandb import (\n",
    "    WandbLoggerCallback,\n",
    "    WandbTrainableMixin,\n",
    "    wandb_mixin,\n",
    ")\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gym  # RL environments and action/observation spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's RecSim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import linregress, sem\n",
    "from starlette.requests import Request\n",
    "import tree  # dm_tree\n",
    "\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {},
   "source": [
    "<a id='recsim'></a>\n",
    "## Introducing google RecSim\n",
    "\n",
    "<img src=\"images/recsim_documentation.png\" width=600 style=\"float:right;\">\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- the \"env\" we will use in this tutorial)\n",
    "* Interest Evolution\n",
    "* Interest Exploration\n",
    "\n",
    "<a id='dissecting_lte'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # action_sapce = Discrete(10) -> int 0-9\n",
    "    \"slate_size\": 1,\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# What are our spaces?\n",
    "#print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "print(f\"action space = {lts_10_1_env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b",
   "metadata": {},
   "source": [
    "Let's make use of our knowledge on the gym.Env API and call our new environment's `reset()` and `step()` methods.\n",
    "First: `reset()` to receive the initial observation in a new episode/trajectory/session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
   "metadata": {},
   "source": [
    "Now let's play RL agent ourselves and recommend some items (pick some actions) via the environment's `step()` method:\n",
    "\n",
    "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {},
   "source": [
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />..<br />.<br />.<br />.<br />.\n",
    "\n",
    "\n",
    "### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {},
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<img src=\"images/long_term_satisfaction_env.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
   "metadata": {},
   "source": [
    "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. We can use this following `gym.ObservationWrapper` class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375a469e-aa46-4038-9df7-02cabccdad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified_lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce752b-68a3-4a84-aada-97810039e4e8",
   "metadata": {},
   "source": [
    "Now that we have a stronger effect of the user's satisfaction value on the long-term rewards, we may be able to measure this effect reliably\n",
    "using the following utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848212-87b8-4eb1-9353-74e09ae72310",
   "metadata": {},
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's find out how well a randomly acting agent performs in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    \"resample_documents\": True,\n",
    "    # Convert to Discrete action space.\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "}))\n",
    "\n",
    "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib\n",
    "<a id='rllib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms_bandits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {},
   "source": [
    "### Trying a \"Contextual n-armed Bandit\" on our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911f212-523e-4a75-846d-342dd2a681a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dicts for RLlib Trainers.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# E.g. Bandit algorithms:\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "print(f\"Bandit's default config is:\")\n",
    "pprint(BANDIT_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    # ... and the Trainer itself.\n",
    "    \"seed\": 0,\n",
    "\n",
    "    # The following settings are affecting the reporting only:\n",
    "    # ---\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {},
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(3000):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = bandit_trainer.train()\n",
    "    # Extract reward from results.\n",
    "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
   "metadata": {},
   "source": [
    "<a id='bandit_results'></a>\n",
    "### What does our trained Bandit actually recommend?\n",
    "\n",
    "The first method of the RLlib Trainer API we used above was `train()`.\n",
    "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
    "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb62acb-0a16-4412-949a-4851201620bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "obs = lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
   "metadata": {},
   "source": [
    "### Ok, Bandits want Chocolate! :)\n",
    "#### Why is that?\n",
    "\n",
    "<img src=\"images/contextual_bandit.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84291b69-050f-489a-b822-239294bb3a2e",
   "metadata": {},
   "source": [
    "### Recap: Advantages and Disatvantages of Bandits:\n",
    "#### Advantages:\n",
    "* Very fast\n",
    "* Very sample-efficient\n",
    "* Easy to understand learning process\n",
    "\n",
    "#### Disadvantages\n",
    "* Need immediate reward (not capable of solving long-horizon credit assignment problem)\n",
    "* Models user -> If > 1 user, must train separate bandit per user\n",
    "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a830b-cc5f-454c-b986-75c59d40df89",
   "metadata": {},
   "source": [
    "<a id='slateq'></a>\n",
    "### Switching to Slate-Q\n",
    "\n",
    "<img src=\"images/rllib_algorithms_slateq.png\" width=800>\n",
    "\n",
    "RLlib offers another algorithm - Slate-Q - designed for k-slate, long time horizon, and dynamic user recommendation problems. Let's take a quick look:\n",
    "\n",
    "<img src=\"images/slateq.png\" width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c258f2-60db-4ed2-8c4f-8f83809d1d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "    },\n",
    "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
    "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
    "    # the course of `epsilon_timesteps`.\n",
    "    \"exploration_config\": {\n",
    "        #\"warmup_timesteps\": 20000,  # default\n",
    "        \"epsilon_timesteps\": tune.grid_search([40000, 2000, 3000]),  # default: 250000\n",
    "    },\n",
    "    #\"learning_starts\": 20000,  # default\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "<a id='slateq_experiment'></a>\n",
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(40):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "\n",
    "while Slate-Q is (hopefully) leaning\n",
    "\n",
    "------------------\n",
    "\n",
    "<a id='slateq_results'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
    "\n",
    "obs = lts_20_2_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"doc\"][str(action[0])][0]\n",
    "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    max_choc_feat = obs['doc'][str(max_feat_action)][0]\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action} max-choc-feature={max_choc_feat}\")\n",
    "    \n",
    "    obs, r, done, _ = lts_20_2_env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = lts_20_2_env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf1390-bdea-412a-be31-ebbbc6f4ac72",
   "metadata": {},
   "source": [
    "#### !END: OPTIONAL HACK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recap: Advantages and Disadvantages of SlateQ:\n",
    "#### Advantages:\n",
    "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
    "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
    "* Handles > 1 user problems\n",
    "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
    "\n",
    "#### Disadvantages\n",
    "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
    "* Slower and heavier feel to it\n",
    "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='offline_rl'></a>\n",
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5114691-b74f-4b4e-8bdb-5df704bee067",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The logdir contains the following files:\n",
      "['README.py', 'output-2022-03-28_16-43-43_worker-2_0.json']\n",
      "\n",
      "\n",
      "The JSON file with all sampled trajectories is:\n",
      "offline_rl/output-2022-03-28_16-43-43_worker-2_0.json\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = \"offline_rl/\"\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"\\n\\nThe logdir contains the following files:\")\n",
    "all_output_files = os.listdir(os.path.dirname(output_dir + \"/\"))\n",
    "pprint(all_output_files)\n",
    "\n",
    "json_output_file = os.path.join(output_dir, [f for f in all_output_files if re.match(\"^.*worker.*\\.json$\", f)][0])\n",
    "print(\"\\n\\nThe JSON file with all sampled trajectories is:\")\n",
    "print(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a0aaea-b811-41e1-9e6c-532d9ce1b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>obs</th>\n",
       "      <th>actions</th>\n",
       "      <th>prev_actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>prev_rewards</th>\n",
       "      <th>dones</th>\n",
       "      <th>eps_id</th>\n",
       "      <th>unroll_id</th>\n",
       "      <th>agent_index</th>\n",
       "      <th>t</th>\n",
       "      <th>action_dist_inputs</th>\n",
       "      <th>action_logp</th>\n",
       "      <th>action_prob</th>\n",
       "      <th>advantages</th>\n",
       "      <th>value_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsI0kAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[18, 2], [2, 15], [11, 14], [8, 16], [4, 18],...</td>\n",
       "      <td>[[18, 2], [18, 2], [2, 15], [11, 14], [8, 16],...</td>\n",
       "      <td>[23.29007911682129, 7.152186393737793, 2.76373...</td>\n",
       "      <td>[23.29007911682129, 23.29007911682129, 7.15218...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[542138261, 542138261, 542138261, 542138261, 5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[[-0.006720410659909, 0.014437448233366, -0.00...</td>\n",
       "      <td>[-5.9771623611450195, -6.004640102386475, -5.9...</td>\n",
       "      <td>[0.0025360123254350004, 0.0024672772269690004,...</td>\n",
       "      <td>[932.8204345703125, 918.7171630859375, 920.772...</td>\n",
       "      <td>[932.8135986328125, 918.7106323242188, 920.766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACs/0cAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[13, 3], [16, 17], [7, 17], [8, 7], [4, 5], [...</td>\n",
       "      <td>[[14, 1], [13, 3], [16, 17], [7, 17], [8, 7], ...</td>\n",
       "      <td>[5.713678359985352, 18.23175048828125, 6.07325...</td>\n",
       "      <td>[21.042491912841797, 5.713678359985352, 18.231...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1118934910, 1118934910, 1118934910, 111893491...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>[[-0.003344316966831, -0.002333797048777, 0.00...</td>\n",
       "      <td>[-6.002190589904785, -5.994460582733154, -5.98...</td>\n",
       "      <td>[0.002473328262567, 0.0024925211910150004, 0.0...</td>\n",
       "      <td>[515.4043579101562, 514.835205078125, 501.6228...</td>\n",
       "      <td>[515.3938598632812, 514.8284301757812, 501.612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsX0kAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[3, 17], [6, 3], [15, 17], [11, 2], [0, 12], ...</td>\n",
       "      <td>[[10, 1], [3, 17], [6, 3], [15, 17], [11, 2], ...</td>\n",
       "      <td>[14.05547046661377, 20.706499099731445, 34.158...</td>\n",
       "      <td>[4.046912670135498, 14.05547046661377, 20.7064...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1466202559, 1466202559, 1466202559, 146620255...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...</td>\n",
       "      <td>[[-0.003262223675847, 0.008641279302537, -0.00...</td>\n",
       "      <td>[-5.993566513061523, -5.996161460876465, -5.98...</td>\n",
       "      <td>[0.002494750544428, 0.0024882853031150003, 0.0...</td>\n",
       "      <td>[221.78857421875, 209.83851623535156, 191.0375...</td>\n",
       "      <td>[221.7843475341797, 209.8271484375, 191.030960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsCkkAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[10, 19], [8, 3], [18, 8], [8, 7], [16, 2], [...</td>\n",
       "      <td>[[10, 19], [10, 19], [8, 3], [18, 8], [8, 7], ...</td>\n",
       "      <td>[34.26883316040039, 21.044775009155273, 8.3825...</td>\n",
       "      <td>[34.26883316040039, 34.26883316040039, 21.0447...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1418911249, 1418911249, 1418911249, 141891124...</td>\n",
       "      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[[-0.008724463172256001, 0.0040771835483610006...</td>\n",
       "      <td>[-5.995693683624268, -5.9909610748291025, -5.9...</td>\n",
       "      <td>[0.002489449456334, 0.002501259092241, 0.00250...</td>\n",
       "      <td>[658.6182861328125, 630.65478515625, 615.76763...</td>\n",
       "      <td>[658.6080322265625, 630.6456298828125, 615.758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsjkkAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[1, 8], [2, 19], [9, 1], [1, 16], [5, 2], [15...</td>\n",
       "      <td>[[6, 5], [1, 8], [2, 19], [9, 1], [1, 16], [5,...</td>\n",
       "      <td>[3.041858196258545, 8.18496322631836, 62.71932...</td>\n",
       "      <td>[3.607390880584717, 3.041858196258545, 8.18496...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[450621694, 450621694, 450621694, 450621694, 4...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>[[-0.002998510375618, -0.00399568863213, -0.00...</td>\n",
       "      <td>[-5.999508380889893, -5.996327877044678, -5.98...</td>\n",
       "      <td>[0.0024799711536610002, 0.0024878710974000004,...</td>\n",
       "      <td>[455.0074157714844, 456.52606201171875, 452.87...</td>\n",
       "      <td>[454.9999084472656, 456.5232849121094, 452.866...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                                obs  \\\n",
       "0  SampleBatch  BCJNGGhAwFEAAAAAAACsI0kAAGGABZW1UQABAPMZjBJudW...   \n",
       "1  SampleBatch  BCJNGGhAwFEAAAAAAACs/0cAAGGABZW1UQABAPMZjBJudW...   \n",
       "2  SampleBatch  BCJNGGhAwFEAAAAAAACsX0kAAGGABZW1UQABAPMZjBJudW...   \n",
       "3  SampleBatch  BCJNGGhAwFEAAAAAAACsCkkAAGGABZW1UQABAPMZjBJudW...   \n",
       "4  SampleBatch  BCJNGGhAwFEAAAAAAACsjkkAAGGABZW1UQABAPMZjBJudW...   \n",
       "\n",
       "                                             actions  \\\n",
       "0  [[18, 2], [2, 15], [11, 14], [8, 16], [4, 18],...   \n",
       "1  [[13, 3], [16, 17], [7, 17], [8, 7], [4, 5], [...   \n",
       "2  [[3, 17], [6, 3], [15, 17], [11, 2], [0, 12], ...   \n",
       "3  [[10, 19], [8, 3], [18, 8], [8, 7], [16, 2], [...   \n",
       "4  [[1, 8], [2, 19], [9, 1], [1, 16], [5, 2], [15...   \n",
       "\n",
       "                                        prev_actions  \\\n",
       "0  [[18, 2], [18, 2], [2, 15], [11, 14], [8, 16],...   \n",
       "1  [[14, 1], [13, 3], [16, 17], [7, 17], [8, 7], ...   \n",
       "2  [[10, 1], [3, 17], [6, 3], [15, 17], [11, 2], ...   \n",
       "3  [[10, 19], [10, 19], [8, 3], [18, 8], [8, 7], ...   \n",
       "4  [[6, 5], [1, 8], [2, 19], [9, 1], [1, 16], [5,...   \n",
       "\n",
       "                                             rewards  \\\n",
       "0  [23.29007911682129, 7.152186393737793, 2.76373...   \n",
       "1  [5.713678359985352, 18.23175048828125, 6.07325...   \n",
       "2  [14.05547046661377, 20.706499099731445, 34.158...   \n",
       "3  [34.26883316040039, 21.044775009155273, 8.3825...   \n",
       "4  [3.041858196258545, 8.18496322631836, 62.71932...   \n",
       "\n",
       "                                        prev_rewards  \\\n",
       "0  [23.29007911682129, 23.29007911682129, 7.15218...   \n",
       "1  [21.042491912841797, 5.713678359985352, 18.231...   \n",
       "2  [4.046912670135498, 14.05547046661377, 20.7064...   \n",
       "3  [34.26883316040039, 34.26883316040039, 21.0447...   \n",
       "4  [3.607390880584717, 3.041858196258545, 8.18496...   \n",
       "\n",
       "                                               dones  \\\n",
       "0  [False, False, False, False, False, False, Fal...   \n",
       "1  [False, False, False, False, False, False, Fal...   \n",
       "2  [False, False, False, False, False, False, Fal...   \n",
       "3  [False, False, False, False, False, False, Fal...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              eps_id  \\\n",
       "0  [542138261, 542138261, 542138261, 542138261, 5...   \n",
       "1  [1118934910, 1118934910, 1118934910, 111893491...   \n",
       "2  [1466202559, 1466202559, 1466202559, 146620255...   \n",
       "3  [1418911249, 1418911249, 1418911249, 141891124...   \n",
       "4  [450621694, 450621694, 450621694, 450621694, 4...   \n",
       "\n",
       "                                           unroll_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "2  [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...   \n",
       "3  [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...   \n",
       "4  [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...   \n",
       "\n",
       "                                         agent_index  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   t  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "2  [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...   \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "4  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "\n",
       "                                  action_dist_inputs  \\\n",
       "0  [[-0.006720410659909, 0.014437448233366, -0.00...   \n",
       "1  [[-0.003344316966831, -0.002333797048777, 0.00...   \n",
       "2  [[-0.003262223675847, 0.008641279302537, -0.00...   \n",
       "3  [[-0.008724463172256001, 0.0040771835483610006...   \n",
       "4  [[-0.002998510375618, -0.00399568863213, -0.00...   \n",
       "\n",
       "                                         action_logp  \\\n",
       "0  [-5.9771623611450195, -6.004640102386475, -5.9...   \n",
       "1  [-6.002190589904785, -5.994460582733154, -5.98...   \n",
       "2  [-5.993566513061523, -5.996161460876465, -5.98...   \n",
       "3  [-5.995693683624268, -5.9909610748291025, -5.9...   \n",
       "4  [-5.999508380889893, -5.996327877044678, -5.98...   \n",
       "\n",
       "                                         action_prob  \\\n",
       "0  [0.0025360123254350004, 0.0024672772269690004,...   \n",
       "1  [0.002473328262567, 0.0024925211910150004, 0.0...   \n",
       "2  [0.002494750544428, 0.0024882853031150003, 0.0...   \n",
       "3  [0.002489449456334, 0.002501259092241, 0.00250...   \n",
       "4  [0.0024799711536610002, 0.0024878710974000004,...   \n",
       "\n",
       "                                          advantages  \\\n",
       "0  [932.8204345703125, 918.7171630859375, 920.772...   \n",
       "1  [515.4043579101562, 514.835205078125, 501.6228...   \n",
       "2  [221.78857421875, 209.83851623535156, 191.0375...   \n",
       "3  [658.6182861328125, 630.65478515625, 615.76763...   \n",
       "4  [455.0074157714844, 456.52606201171875, 452.87...   \n",
       "\n",
       "                                       value_targets  \n",
       "0  [932.8135986328125, 918.7106323242188, 920.766...  \n",
       "1  [515.3938598632812, 514.8284301757812, 501.612...  \n",
       "2  [221.7843475341797, 209.8271484375, 191.030960...  \n",
       "3  [658.6080322265625, 630.6456298828125, 615.758...  \n",
       "4  [454.9999084472656, 456.5232849121094, 452.866...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the output file first:\n",
    "dataframe = pandas.read_json(json_output_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48768f8f-25ad-4fee-92d2-5f99f34295f9",
   "metadata": {},
   "source": [
    "<a id='bc_and_marwil'></a>\n",
    "### Picking an offline RL algorithm\n",
    "\n",
    "RLlib offers different offline specialized algorithms, such as Behavior Cloning (imitation learning), MARWIL, or CQL.\n",
    "\n",
    "<img src=\"images/rllib_algorithms_offline_rl.png\" width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e7b767-d11e-4b2d-bf86-bf88f06f8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ray.tune.utils.util:Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCTrainer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil import BCTrainer\n",
    "\n",
    "offline_rl_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,\n",
    "    \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "    \"convert_to_discrete_action_space\": False,\n",
    "})\n",
    "\n",
    "\n",
    "# Configuring the BCTrainer:\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    \"actions_in_input_normalized\": True,\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "\n",
    "    # Since we don't have an environment and the obs/action-spaces are not defined in the JSON file,\n",
    "    # we need to provide these here manually.\n",
    "    \"env\": None,  # default\n",
    "    \"observation_space\": offline_rl_env.observation_space,\n",
    "    \"action_space\": offline_rl_env.action_space,\n",
    "\n",
    "    # Perform \"off-policy estimation\" (OPE) on train batches and report results.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "}\n",
    "\n",
    "# Create a behavior cloning (BC) Trainer from our config.\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(10):\n",
    "    results = bc_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; reward = {results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48b13b-5b2d-4e75-9b50-c1a6b45c253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# For behavior cloning, simply looking at the loss as a measurement of progress would be a good idea:\n",
    "for _ in range(10):\n",
    "    results = bc_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; loss = {results['info']['learner']['default_policy']['learner_stats']['total_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59de0fb-9c6b-4079-9357-f92874730a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf492b2-d525-4a73-9a10-e2017a61adaa",
   "metadata": {},
   "source": [
    "### Training BC trainer with Ray tune + WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be29623-6bd4-4d9a-85f6-d1a0df4926ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official Ray example\n",
    "\n",
    "# api_key_file = \"~/.wandb_api_key\"\n",
    "\n",
    "# class WandbTrainable(WandbTrainableMixin, Trainable):\n",
    "#     def step(self):\n",
    "#         for i in range(30):\n",
    "#             loss = self.config[\"mean\"] + self.config[\"sd\"] * np.random.randn()\n",
    "#             wandb.log({\"loss\": loss})\n",
    "#         return {\"loss\": loss, \"done\": True}\n",
    "        \n",
    "        \n",
    "# def tune_trainable(api_key_file):\n",
    "#     \"\"\"Example for using a WandTrainableMixin with the class API\"\"\"\n",
    "#     analysis = tune.run(\n",
    "#         WandbTrainable,\n",
    "#         metric=\"loss\",\n",
    "#         mode=\"min\",\n",
    "#         config={\n",
    "#             \"mean\": tune.grid_search([1, 2, 3, 4, 5]),\n",
    "#             \"sd\": tune.uniform(0.2, 0.8),\n",
    "#             \"wandb\": {\"api_key_file\": api_key_file, \"project\": \"Wandb_example\"},\n",
    "#         },\n",
    "#     )\n",
    "#     return analysis.best_config\n",
    "\n",
    "\n",
    "# tune_trainable(api_key_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe860da1-22c7-4f8a-8e61-52738fe019de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--+ 1 zla0368 str.users 9.6M Apr 19 08:42 /disk1/zla0368_v2/caitee/wandb/rllib_tutorials_with_wandb/production_rl_2022/offline_rl/output-2022-03-28_16-43-43_worker-2_0.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lht /disk1/zla0368_v2/caitee/wandb/rllib_tutorials_with_wandb/production_rl_2022/offline_rl/output-2022-03-28_16-43-43_worker-2_0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71c24a-33b5-499b-8ca1-c64708bb9d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WandbLoggerCallback.__del__ at 0x7f54ee3eb9d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/proj/aaco/team/heng/env/miniconda3/envs/rllib_tutorial/lib/python3.9/site-packages/ray/tune/integration/wandb.py\", line 393, in __del__\n",
      "    for trial in self._trial_processes:\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    }
   ],
   "source": [
    "# Offline RL training on LongTermSatisfactionRecSimEnv with wandb\n",
    "\n",
    "api_key_file = \"~/.wandb_api_key\"\n",
    "\n",
    "\n",
    "from ray.rllib.agents.marwil import BCTrainer\n",
    "\n",
    "offline_rl_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,\n",
    "    \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "    \"convert_to_discrete_action_space\": False,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Configuring the BCTrainer:\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    \"actions_in_input_normalized\": True,\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "\n",
    "    # Since we don't have an environment and the obs/action-spaces are not defined in the JSON file,\n",
    "    # we need to provide these here manually.\n",
    "    \"env\": None,  # default\n",
    "    \"observation_space\": offline_rl_env.observation_space,\n",
    "    \"action_space\": offline_rl_env.action_space,\n",
    "\n",
    "    # Perform \"off-policy estimation\" (OPE) on train batches and report results.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "}\n",
    "\n",
    "\n",
    "def tune_function(api_key_file):\n",
    "    \"\"\"Example for using a WandbLoggerCallback with the function API\"\"\"\n",
    "    analysis = tune.run(\n",
    "        BCTrainer,\n",
    "        metric=\"info/learner/default_policy/learner_stats/total_loss\",\n",
    "        mode=\"min\",\n",
    "        config=offline_rl_config,\n",
    "        callbacks=[\n",
    "            WandbLoggerCallback(api_key_file=api_key_file, project=\"Offline_RL_Wandb_example\")\n",
    "        ],\n",
    "    )\n",
    "    return analysis.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40f620-d465-4d11-a5c1-999929c0bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_function(api_key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5cebc-bd70-4bc0-a649-dc33d4e0d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb18e474-c5d0-45be-97e3-932482bc1cbb",
   "metadata": {},
   "source": [
    "<a id='ope'></a>\n",
    "### Off Policy Estimators\n",
    "Also: `results` still holds the last output of our non-evaluation BCTrainer.\n",
    "\n",
    "Extract off-policy estimator (OPE) results for the different methods:\n",
    "* 'is'=importance sampling\n",
    "<img src=\"images/ope.png\" width=400>\n",
    "\n",
    "* 'wis'=weighted importance sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41f0d5-7388-42b3-bd49-e82789011d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ope_results = results['off_policy_estimator']\n",
    "print(f\"IS off-policy estimation: {ope_results['is']}\")\n",
    "print(f\"WIS off-policy estimation: {ope_results['wis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `bc_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = bc_trainer.save()\n",
    "print(f\"Trainer (at iteration {bc_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_bc_trainer.iteration}\")\n",
    "new_bc_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_bc_trainer.iteration}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147db25-4202-433e-aadb-c4fadbdfcbea",
   "metadata": {},
   "source": [
    "<a id='ray_serve'></a>\n",
    "### Deploying a trained policy via Ray Serve\n",
    "In the following cell, we'll learn how to use Ray Serve to deploy a trained policy, e.g. in your production for inference (computing actions; no further learning):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadb612-9578-46de-a9e8-67724045c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.start()\n",
    "\n",
    "@serve.deployment(route_prefix=\"/long-term-satisfaction\")\n",
    "class ServeModel:\n",
    "    def __init__(self, checkpoint_path) -> None:\n",
    "        self.trainer = BCTrainer(\n",
    "            config=offline_rl_config,\n",
    "        )\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = OrderedDict(tree.map_structure(lambda s: np.array(s) if isinstance(s, list) else s, obs))\n",
    "        action = self.trainer.compute_single_action(np_obs, explore=False)\n",
    "        return {\"action\": action}\n",
    "\n",
    "\n",
    "ServeModel.deploy(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082340d-51f4-42a2-853a-508413a3d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request 5 actions of an episode from served policy.\n",
    "obs = offline_rl_env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    # Convert numpy arrays to lists (needed for transfer).\n",
    "    obs = tree.map_structure(lambda s: s.tolist() if isinstance(s, np.ndarray) else s, obs)\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/long-term-satisfaction\", json={\"observation\": obs}\n",
    "    )\n",
    "    response_json = resp.json()\n",
    "    print(f\"<- got {response_json}\")\n",
    "    obs, _, _, _ = offline_rl_env.step(np.array(response_json[\"action\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
